{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(fname):\n",
    "  return fname.split('_')[1]\n",
    "\n",
    "def get_sub(fname):\n",
    "  return fname.split('_')[0]\n",
    "\n",
    "def integrate_video(fname):\n",
    "    img = Image.open('./image dataset/' + fname).convert('CMYK')\n",
    "    data = np.expand_dims(np.asarray(img), axis=0)\n",
    "    for i in range(1, 7):\n",
    "        fn = fname.replace('_1.jpg',f'_{i+1}.jpg')\n",
    "        img_ = Image.open('./image dataset/' + fn).convert('CMYK')\n",
    "        arr = np.expand_dims(np.asarray(img_), axis=0)\n",
    "        data = np.concatenate((data, arr), axis=0)  # shape: (frame_num, width, height, channel)\n",
    "    return data\n",
    "\n",
    "def save_video(files):\n",
    "    ds = []\n",
    "    sub_dict = []\n",
    "    class_dict = []\n",
    "    for f in files:\n",
    "        data = integrate_video(f)\n",
    "        sub = get_sub(f)\n",
    "        cls = get_class(f)\n",
    "        ds.append(data)\n",
    "        sub_dict.append(sub)\n",
    "        class_dict.append(cls)\n",
    "\n",
    "    return ds, sub_dict, class_dict\n",
    "\n",
    "def get_files_per_class(files):\n",
    "  files_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    files_for_class[class_name].append(fname)\n",
    "  return files_for_class\n",
    "\n",
    "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
    "  \"\"\" \n",
    "    Use for 3-class/in-subclass comparison\n",
    "  \"\"\"\n",
    "  files_subset = dict()\n",
    "\n",
    "  for class_name in classes:\n",
    "    class_files = files_for_class[class_name]\n",
    "    files_subset[class_name] = class_files[:files_per_class]\n",
    "\n",
    "  return files_subset\n",
    "\n",
    "def get_files_per_sub_per_class(files):\n",
    "  files_for_sub_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    sub_num = get_sub(fname)\n",
    "    files_for_sub_for_class[(sub_num, class_name)].append(fname)\n",
    "  return files_for_sub_for_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Tianyi Zheng\\AppData\\Local\\Temp\\ipykernel_22520\\1526345196.py:4: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  files_head = [f for f in files if re.match(r'^sub' + str(sub_num) + '.*_1\\.jpg$', f)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 6\n",
      "Num videos for class[0]: 2205\n",
      "Num classes * subjects: 42\n",
      "Num videos for class[0]: 315\n"
     ]
    }
   ],
   "source": [
    "path = './image dataset/'\n",
    "files = os.listdir(path)\n",
    "sub_num = 5\n",
    "files_head = [f for f in files if re.match(r'^sub' + str(sub_num) + '.*_1\\.jpg$', f)]\n",
    "\n",
    "data, sub_dict, class_dict = save_video(files_head)\n",
    "\n",
    "files_for_class = get_files_per_class(files)\n",
    "classes = list(files_for_class.keys())\n",
    "\n",
    "print('Num classes:', len(classes))\n",
    "print('Num videos for class[0]:', len(files_for_class[classes[0]]))\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "FILES_PER_CLASS = 90   # 270 trials for each sub, 45 trials for each class per sub, 315 trials for each class in general\n",
    "\n",
    "files_subset = select_subset_of_classes(files_for_class, classes[:NUM_CLASSES], FILES_PER_CLASS)\n",
    "list(files_subset.keys())\n",
    "\n",
    "files_for_sub_for_class = get_files_per_sub_per_class(files)\n",
    "classes_2stage = list(files_for_sub_for_class.keys())\n",
    "\n",
    "print('Num classes * subjects:', len(classes_2stage))\n",
    "print('Num videos for class[0]:', len(files_for_sub_for_class[classes_2stage[0]]))\n",
    "\n",
    "trial_dict = {'front':0, 'back':1, 'left':2, 'right':3, 'up':4, 'down':5}\n",
    "label = [trial_dict[x] for x in class_dict]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class video_classification_dataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = [torch.tensor(x, dtype=torch.float32) for x in x_data]\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset = video_classification_dataset(x_train, y_train)\n",
    "test_dataset = video_classification_dataset(x_test, y_test)\n",
    "\n",
    "# Creating dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Now train_dataloader and test_dataloader are ready to be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 369\n",
    "WIDTH = 433\n",
    "input_shape = (None, 7, HEIGHT, WIDTH, 4)\n",
    "\n",
    "def calculate_padding(kernel_size, stride, dilation):\n",
    "    return ((kernel_size - 1) * dilation + 1 - stride) // 2\n",
    "\n",
    "# Define Custom Layers and Model\n",
    "class Conv2Plus1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(Conv2Plus1D, self).__init__()\n",
    "        # Spatial decomposition\n",
    "        spatial_kernal = (1, kernel_size[1], kernel_size[2])\n",
    "        self.spatial_conv = nn.Conv3d(in_channels=in_channels, \n",
    "                                      out_channels=out_channels, \n",
    "                                      kernel_size=spatial_kernal, \n",
    "                                      stride=stride, \n",
    "                                      padding=tuple(calculate_padding(k, stride=1, dilation=1) for k in spatial_kernal))\n",
    "        # Temporal decomposition\n",
    "        temporal_kernal =(kernel_size[0], 1, 1)\n",
    "        self.temporal_conv = nn.Conv3d(in_channels=out_channels, \n",
    "                                       out_channels=out_channels, \n",
    "                                       kernel_size=temporal_kernal, \n",
    "                                       stride=stride, \n",
    "                                       padding=tuple(calculate_padding(k, stride=1, dilation=1) for k in temporal_kernal))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"start conv2plus1d\")\n",
    "        # print(x.shape)\n",
    "        x = self.spatial_conv(x)\n",
    "        # print(\"spatial conv done\")\n",
    "        x = self.temporal_conv(x)\n",
    "        # print(\"temporal conv done\")\n",
    "        return x\n",
    "\n",
    "class ResidualMain(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, current_height, current_width):\n",
    "        super(ResidualMain, self).__init__()\n",
    "        self.conv1 = Conv2Plus1D(in_channels=in_channels, \n",
    "                                 out_channels=out_channels, \n",
    "                                 kernel_size=kernel_size, \n",
    "                                 stride=stride)\n",
    "        self.bn1 = nn.LayerNorm([out_channels, 7, current_height, current_width])  # Adjust the size accordingly ????\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = Conv2Plus1D(in_channels=out_channels, \n",
    "                                 out_channels=out_channels, \n",
    "                                 kernel_size=kernel_size, \n",
    "                                 stride=stride)\n",
    "        self.bn2 = nn.LayerNorm([out_channels, 7,current_height, current_width]) # ???\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        return out\n",
    "\n",
    "class Project(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Project, self).__init__()\n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "        self.bn = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "def add_residual_block(input, in_channels, out_channels, kernel_size, stride):\n",
    "    out = ResidualMain(in_channels, out_channels, kernel_size, stride)(input)\n",
    "    res = input\n",
    "    if out.shape != input.shape:\n",
    "        res = Project(input.shape[-1], out.shape[-1])(res)\n",
    "    return out + res\n",
    "\n",
    "\n",
    "class ResizeVideo(nn.Module):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, video):\n",
    "        \"\"\"\n",
    "        Resize the tensor representing a video to a new height and width.\n",
    "\n",
    "        Args:\n",
    "          video: Tensor representation of the video, in the form of [batch_size, channels, frames, height, width]\n",
    "\n",
    "        Return:\n",
    "          Resized video tensor: [batch_size, channels, frames, new_height, new_width]\n",
    "        \"\"\"\n",
    "        # Parse the original shape\n",
    "        old_shape = einops.parse_shape(video, 'b c t h w')\n",
    "        \n",
    "        # First, permute the video to [batch_size, frames, height, width, channels] \n",
    "        video = video.permute(0, 2, 3, 4, 1)  # From [b, c, t, h, w] to [b, t, h, w, c]\n",
    "        \n",
    "        # Reshape the video to [(batch_size * frames), height, width, channels]\n",
    "        images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "        \n",
    "        # Permute for interpolation: [batch_size * frames, channels, height, width]\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Resize the images using torch.nn.functional.interpolate\n",
    "        resized_images = F.interpolate(images, size=(self.height, self.width), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Permute back to [(batch_size * frames), height, width, channels]\n",
    "        resized_images = resized_images.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # Reshape back to [batch_size, frames, new_height, new_width, channels]\n",
    "        videos = einops.rearrange(resized_images, '(b t) h w c -> b t h w c', t=old_shape['t'])\n",
    "        \n",
    "        # Finally, permute back to the original format: [batch_size, channels, frames, new_height, new_width]\n",
    "        videos = videos.permute(0, 4, 1, 2, 3)  # From [b, t, h, w, c] to [b, c, t, h, w]\n",
    "        \n",
    "        return videos\n",
    "\n",
    "\n",
    "class VideoClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoClassificationModel, self).__init__()\n",
    "        self.conv1 = Conv2Plus1D(in_channels=4, \n",
    "                                 out_channels=16, \n",
    "                                 kernel_size=(3, 7, 7), \n",
    "                                 stride=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.resize1 = ResizeVideo(HEIGHT // 2, WIDTH // 2)\n",
    "\n",
    "        # Residual blocks\n",
    "        # block 1\n",
    "        self.block1 = ResidualMain(16, 16, (3, 3, 3), (1, 1, 1), \n",
    "                                   current_height=HEIGHT // 2, current_width=WIDTH // 2)\n",
    "        self.resize2 = ResizeVideo(HEIGHT // 4, WIDTH // 4)\n",
    "        # block 2\n",
    "        self.block2 = ResidualMain(16, 32, (3, 3, 3), (1, 1, 1),\n",
    "                                   current_height=HEIGHT // 4, current_width=WIDTH // 4)\n",
    "        self.resize3 = ResizeVideo(HEIGHT // 8, WIDTH // 8)\n",
    "        # block 3\n",
    "        self.block3 = ResidualMain(32, 64, (3, 3, 3), (1, 1, 1),\n",
    "                                   current_height=HEIGHT // 8, current_width=WIDTH // 8)\n",
    "        self.resize4 = ResizeVideo(HEIGHT // 16, WIDTH // 16)\n",
    "        # block 4\n",
    "        self.block4 = ResidualMain(64, 128, (3, 3, 3), (1, 1, 1),\n",
    "                                   current_height=HEIGHT // 16, current_width=WIDTH // 16)\n",
    "\n",
    "        # Classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"start\")\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # From [8, 7, 369, 433, 4](b t h w c) -> [8, 4, 7, 369, 433](b c t h w)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"conv1 done.\")\n",
    "        x = self.bn1(x)\n",
    "        # print(\"bn1 done.\")\n",
    "        x = self.relu(x)\n",
    "        # print(\"relu done.\")\n",
    "        x = self.resize1(x)\n",
    "        # print(\"resize1 done.\")\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = self.block1(x)\n",
    "        # print(\"block1 done.\")\n",
    "        x = self.resize2(x)\n",
    "        # print(\"resize2 done.\")\n",
    "        x = self.block2(x)\n",
    "        # print(\"block2 done.\")\n",
    "        x = self.resize3(x)\n",
    "        # print(\"resize3 done.\")\n",
    "        x = self.block3(x)\n",
    "        # print(\"block3 done.\")\n",
    "        x = self.resize4(x)\n",
    "        # print(\"resize4 done.\")\n",
    "        x = self.block4(x)\n",
    "        # print(\"block4 done.\")\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        # print(\"global pool done.\")\n",
    "        x = x.view(x.size(0), -1) # flatten ??\n",
    "        # print(\"flatten done.\")\n",
    "        x = self.fc(x)\n",
    "        # print(\"linear done\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "model = VideoClassificationModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# summary(model, input_size=(7, 369, 433, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Loss: 1.9020\n",
      "Epoch [2/60], Loss: 1.8358\n",
      "Epoch [3/60], Loss: 1.8099\n",
      "Epoch [4/60], Loss: 1.7989\n",
      "Epoch [5/60], Loss: 1.7964\n",
      "Epoch [6/60], Loss: 1.8126\n",
      "Epoch [7/60], Loss: 1.7876\n",
      "Epoch [8/60], Loss: 1.8225\n",
      "Epoch [9/60], Loss: 1.7989\n",
      "Epoch [10/60], Loss: 1.8250\n",
      "Epoch [11/60], Loss: 1.8006\n",
      "Epoch [12/60], Loss: 1.7968\n",
      "Epoch [13/60], Loss: 1.7946\n",
      "Epoch [14/60], Loss: 1.7941\n",
      "Epoch [15/60], Loss: 1.8016\n",
      "Epoch [16/60], Loss: 1.7952\n",
      "Epoch [17/60], Loss: 1.7981\n",
      "Epoch [18/60], Loss: 1.7864\n",
      "Epoch [19/60], Loss: 1.7988\n",
      "Epoch [20/60], Loss: 1.7936\n",
      "Epoch [21/60], Loss: 1.7937\n",
      "Epoch [22/60], Loss: 1.7913\n",
      "Epoch [23/60], Loss: 1.7948\n",
      "Epoch [24/60], Loss: 1.7912\n",
      "Epoch [25/60], Loss: 1.7884\n",
      "Epoch [26/60], Loss: 1.7911\n",
      "Epoch [27/60], Loss: 1.7840\n",
      "Epoch [28/60], Loss: 1.7830\n",
      "Epoch [29/60], Loss: 1.7817\n",
      "Epoch [30/60], Loss: 1.7783\n",
      "Epoch [31/60], Loss: 1.7810\n",
      "Epoch [32/60], Loss: 1.7711\n",
      "Epoch [33/60], Loss: 1.7825\n",
      "Epoch [34/60], Loss: 1.8060\n",
      "Epoch [35/60], Loss: 1.7764\n",
      "Epoch [36/60], Loss: 1.7675\n",
      "Epoch [37/60], Loss: 1.7619\n",
      "Epoch [38/60], Loss: 1.7634\n",
      "Epoch [39/60], Loss: 1.7374\n",
      "Epoch [40/60], Loss: 1.7317\n",
      "Epoch [41/60], Loss: 1.7217\n",
      "Epoch [42/60], Loss: 1.6897\n",
      "Epoch [43/60], Loss: 1.7085\n",
      "Epoch [44/60], Loss: 1.6920\n",
      "Epoch [45/60], Loss: 1.6602\n",
      "Epoch [46/60], Loss: 1.6568\n",
      "Epoch [47/60], Loss: 1.6367\n",
      "Epoch [48/60], Loss: 1.6097\n",
      "Epoch [49/60], Loss: 1.5741\n",
      "Epoch [50/60], Loss: 1.6401\n",
      "Epoch [51/60], Loss: 1.5483\n",
      "Epoch [52/60], Loss: 1.5279\n",
      "Epoch [53/60], Loss: 1.5150\n",
      "Epoch [54/60], Loss: 1.4782\n",
      "Epoch [55/60], Loss: 1.4576\n",
      "Epoch [56/60], Loss: 1.4164\n",
      "Epoch [57/60], Loss: 1.4771\n",
      "Epoch [58/60], Loss: 1.4401\n",
      "Epoch [59/60], Loss: 1.4241\n",
      "Epoch [60/60], Loss: 1.3739\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for videos, labels in train_dataloader:\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch [1/60], Loss: 1.9020\n",
    "Epoch [2/60], Loss: 1.8358\n",
    "Epoch [3/60], Loss: 1.8099\n",
    "Epoch [4/60], Loss: 1.7989\n",
    "Epoch [5/60], Loss: 1.7964\n",
    "Epoch [6/60], Loss: 1.8126\n",
    "Epoch [7/60], Loss: 1.7876\n",
    "Epoch [8/60], Loss: 1.8225\n",
    "Epoch [9/60], Loss: 1.7989\n",
    "Epoch [10/60], Loss: 1.8250\n",
    "Epoch [11/60], Loss: 1.8006\n",
    "Epoch [12/60], Loss: 1.7968\n",
    "Epoch [13/60], Loss: 1.7946\n",
    "Epoch [14/60], Loss: 1.7941\n",
    "Epoch [15/60], Loss: 1.8016\n",
    "Epoch [16/60], Loss: 1.7952\n",
    "Epoch [17/60], Loss: 1.7981\n",
    "Epoch [18/60], Loss: 1.7864\n",
    "Epoch [19/60], Loss: 1.7988\n",
    "Epoch [20/60], Loss: 1.7936\n",
    "Epoch [21/60], Loss: 1.7937\n",
    "Epoch [22/60], Loss: 1.7913\n",
    "Epoch [23/60], Loss: 1.7948\n",
    "Epoch [24/60], Loss: 1.7912\n",
    "Epoch [25/60], Loss: 1.7884\n",
    "Epoch [26/60], Loss: 1.7911\n",
    "Epoch [27/60], Loss: 1.7840\n",
    "Epoch [28/60], Loss: 1.7830\n",
    "Epoch [29/60], Loss: 1.7817\n",
    "Epoch [30/60], Loss: 1.7783\n",
    "Epoch [31/60], Loss: 1.7810\n",
    "Epoch [32/60], Loss: 1.7711\n",
    "Epoch [33/60], Loss: 1.7825\n",
    "Epoch [34/60], Loss: 1.8060\n",
    "Epoch [35/60], Loss: 1.7764\n",
    "Epoch [36/60], Loss: 1.7675\n",
    "Epoch [37/60], Loss: 1.7619\n",
    "Epoch [38/60], Loss: 1.7634\n",
    "Epoch [39/60], Loss: 1.7374\n",
    "Epoch [40/60], Loss: 1.7317\n",
    "Epoch [41/60], Loss: 1.7217\n",
    "Epoch [42/60], Loss: 1.6897\n",
    "Epoch [43/60], Loss: 1.7085\n",
    "Epoch [44/60], Loss: 1.6920\n",
    "Epoch [45/60], Loss: 1.6602\n",
    "Epoch [46/60], Loss: 1.6568\n",
    "Epoch [47/60], Loss: 1.6367\n",
    "Epoch [48/60], Loss: 1.6097\n",
    "Epoch [49/60], Loss: 1.5741\n",
    "Epoch [50/60], Loss: 1.6401\n",
    "Epoch [51/60], Loss: 1.5483\n",
    "Epoch [52/60], Loss: 1.5279\n",
    "Epoch [53/60], Loss: 1.5150\n",
    "Epoch [54/60], Loss: 1.4782\n",
    "Epoch [55/60], Loss: 1.4576\n",
    "Epoch [56/60], Loss: 1.4164\n",
    "Epoch [57/60], Loss: 1.4771\n",
    "Epoch [58/60], Loss: 1.4401\n",
    "Epoch [59/60], Loss: 1.4241\n",
    "Epoch [60/60], Loss: 1.3739\n",
    "Training complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
