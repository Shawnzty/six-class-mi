{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda environment eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import mat73\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import einops\n",
    "from scipy.io import loadmat\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "# import PIL.Image as Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# the following import is required for matplotlib < 3.2:\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg') # use a non-interactive backend such as Agg (for PNGs), PDF, SVG or PS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "# using CNN with TensorFlow core\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model, layers\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "# from tensorflow.keras import datasets, layers, models\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "# TensorFlow version: 2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_device():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './image dataset/'\n",
    "files = os.listdir(path)\n",
    "sub_num = 4\n",
    "files_head = [f for f in files if re.match(r'^sub' + str(sub_num) + '.*_1\\.jpg$', f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(fname):\n",
    "  return fname.split('_')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub(fname):\n",
    "  return fname.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_video(fname):\n",
    "    img = Image.open('./image dataset/' + fname).convert('CMYK')\n",
    "    data = np.expand_dims(np.asarray(img), axis=0)\n",
    "    for i in range(1, 7):\n",
    "        fn = fname.replace('_1.jpg',f'_{i+1}.jpg')\n",
    "        img_ = Image.open('./image dataset/' + fn).convert('CMYK')\n",
    "        arr = np.expand_dims(np.asarray(img_), axis=0)\n",
    "        data = np.concatenate((data, arr), axis=0)  # shape: (frame_num, width, height, channel)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(files):\n",
    "    ds = []\n",
    "    sub_dict = []\n",
    "    class_dict = []\n",
    "    for f in files:\n",
    "        data = integrate_video(f)\n",
    "        sub = get_sub(f)\n",
    "        cls = get_class(f)\n",
    "        ds.append(data)\n",
    "        sub_dict.append(sub)\n",
    "        class_dict.append(cls)\n",
    "\n",
    "    return ds, sub_dict, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sub_dict, class_dict = save_video(files_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image: read as video frames (Tensorflow tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data depends on class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_per_class(files):\n",
    "  files_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    files_for_class[class_name].append(fname)\n",
    "  return files_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_for_class = get_files_per_class(files)\n",
    "classes = list(files_for_class.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 6\n",
      "Num videos for class[0]: 2205\n"
     ]
    }
   ],
   "source": [
    "print('Num classes:', len(classes))\n",
    "print('Num videos for class[0]:', len(files_for_class[classes[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
    "  \"\"\" \n",
    "    Use for 3-class/in-subclass comparison\n",
    "  \"\"\"\n",
    "  files_subset = dict()\n",
    "\n",
    "  for class_name in classes:\n",
    "    class_files = files_for_class[class_name]\n",
    "    files_subset[class_name] = class_files[:files_per_class]\n",
    "\n",
    "  return files_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "FILES_PER_CLASS = 90   # 270 trials for each sub, 45 trials for each class per sub, 315 trials for each class in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['back', 'down']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_subset = select_subset_of_classes(files_for_class, classes[:NUM_CLASSES], FILES_PER_CLASS)\n",
    "list(files_subset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data depends on subject and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_per_sub_per_class(files):\n",
    "  files_for_sub_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    sub_num = get_sub(fname)\n",
    "    files_for_sub_for_class[(sub_num, class_name)].append(fname)\n",
    "  return files_for_sub_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_for_sub_for_class = get_files_per_sub_per_class(files)\n",
    "classes_2stage = list(files_for_sub_for_class.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 42\n",
      "Num videos for class[0]: 315\n"
     ]
    }
   ],
   "source": [
    "print('Num classes:', len(classes_2stage))\n",
    "print('Num videos for class[0]:', len(files_for_sub_for_class[classes_2stage[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select random train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dict = {'front':0, 'back':1, 'left':2, 'right':3, 'up':4, 'down':5}\n",
    "label = [trial_dict[x] for x in class_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [tf.convert_to_tensor(item) for item in x_train]\n",
    "x_test = [tf.convert_to_tensor(item) for item in x_test]\n",
    "y_train = [tf.convert_to_tensor(item) for item in y_train]\n",
    "y_test = [tf.convert_to_tensor(item) for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = tf.data.experimental.cardinality(test_ds) # return the num of test batches\n",
    "val_ds = test_ds.take(test_batches//3)\n",
    "test_ds = test_ds.skip(test_batches//3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 369\n",
    "WIDTH = 433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "  \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "  \n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "      \n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "      \n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, 7, HEIGHT, WIDTH, 4)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(6)(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_ds))\n",
    "model.build(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 7, 369, 433  0           []                               \n",
      "                                , 4)]                                                             \n",
      "                                                                                                  \n",
      " conv2_plus1d (Conv2Plus1D)     (None, 7, 369, 433,  3936        ['input_1[0][0]']                \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 7, 369, 433,  64         ['conv2_plus1d[0][0]']           \n",
      " alization)                      16)                                                              \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 7, 369, 433,  0           ['batch_normalization[0][0]']    \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " resize_video (ResizeVideo)     (None, 7, 184, 216,  0           ['re_lu[0][0]']                  \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " residual_main (ResidualMain)   (None, 7, 184, 216,  6272        ['resize_video[0][0]']           \n",
      "                                 16)                                                              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 7, 184, 216,  0           ['resize_video[0][0]',           \n",
      "                                 16)                              'residual_main[0][0]']          \n",
      "                                                                                                  \n",
      " resize_video_1 (ResizeVideo)   (None, 7, 92, 108,   0           ['add[0][0]']                    \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " project (Project)              (None, 7, 92, 108,   608         ['resize_video_1[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " residual_main_1 (ResidualMain)  (None, 7, 92, 108,   20224      ['resize_video_1[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 7, 92, 108,   0           ['project[0][0]',                \n",
      "                                32)                               'residual_main_1[0][0]']        \n",
      "                                                                                                  \n",
      " resize_video_2 (ResizeVideo)   (None, 7, 46, 54, 3  0           ['add_1[0][0]']                  \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " project_1 (Project)            (None, 7, 46, 54, 6  2240        ['resize_video_2[0][0]']         \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " residual_main_2 (ResidualMain)  (None, 7, 46, 54, 6  80384      ['resize_video_2[0][0]']         \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 7, 46, 54, 6  0           ['project_1[0][0]',              \n",
      "                                4)                                'residual_main_2[0][0]']        \n",
      "                                                                                                  \n",
      " resize_video_3 (ResizeVideo)   (None, 7, 23, 27, 6  0           ['add_2[0][0]']                  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " project_2 (Project)            (None, 7, 23, 27, 1  8576        ['resize_video_3[0][0]']         \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " residual_main_3 (ResidualMain)  (None, 7, 23, 27, 1  320512     ['resize_video_3[0][0]']         \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 7, 23, 27, 1  0           ['project_2[0][0]',              \n",
      "                                28)                               'residual_main_3[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling3d (Glob  (None, 128)         0           ['add_3[0][0]']                  \n",
      " alAveragePooling3D)                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['global_average_pooling3d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6)            774         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 443,590\n",
      "Trainable params: 443,558\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./sub3_training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True, # save the entire model or weights only\n",
    "                                                 verbose=1,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',  # max for acc, min for loss\n",
    "                                                 save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9720 - accuracy: 0.2056 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.18750, saving model to ./sub3_training_1\\cp.ckpt\n",
      "23/23 [==============================] - 377s 16s/step - loss: 1.9720 - accuracy: 0.2056 - val_loss: 2.0436 - val_accuracy: 0.1875\n",
      "Epoch 2/60\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8883 - accuracy: 0.2222 \n",
      "Epoch 2: val_accuracy did not improve from 0.18750\n",
      "23/23 [==============================] - 388s 17s/step - loss: 1.8883 - accuracy: 0.2222 - val_loss: 2.1217 - val_accuracy: 0.1562\n",
      "Epoch 3/60\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9595 - accuracy: 0.1944 \n",
      "Epoch 3: val_accuracy did not improve from 0.18750\n",
      "23/23 [==============================] - 375s 16s/step - loss: 1.9595 - accuracy: 0.1944 - val_loss: 2.0360 - val_accuracy: 0.1875\n",
      "Epoch 4/60\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9756 - accuracy: 0.1611 \n",
      "Epoch 4: val_accuracy did not improve from 0.18750\n",
      "23/23 [==============================] - 374s 16s/step - loss: 1.9756 - accuracy: 0.1611 - val_loss: 2.0379 - val_accuracy: 0.1875\n",
      "Epoch 5/60\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9376 - accuracy: 0.1444 \n",
      "Epoch 5: val_accuracy improved from 0.18750 to 0.21875, saving model to ./sub3_training_1\\cp.ckpt\n",
      "23/23 [==============================] - 374s 16s/step - loss: 1.9376 - accuracy: 0.1444 - val_loss: 2.0483 - val_accuracy: 0.2188\n",
      "Epoch 6/60\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9122 - accuracy: 0.1500 \n",
      "Epoch 6: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 382s 17s/step - loss: 1.9122 - accuracy: 0.1500 - val_loss: 1.9913 - val_accuracy: 0.2188\n",
      "Epoch 7/60\n",
      " 5/23 [=====>........................] - ETA: 4:26 - loss: 2.0059 - accuracy: 0.1250"
     ]
    }
   ],
   "source": [
    "# print('hi')\n",
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 60,\n",
    "                    validation_data = val_ds,\n",
    "                    callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the training is interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.9122 - accuracy: 0.1500 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.21875, saving model to ./sub3_training_1\\cp.ckpt\n",
      "23/23 [==============================] - 373s 16s/step - loss: 1.9122 - accuracy: 0.1500 - val_loss: 1.9913 - val_accuracy: 0.2188\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8636 - accuracy: 0.1667 \n",
      "Epoch 2: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 368s 16s/step - loss: 1.8636 - accuracy: 0.1667 - val_loss: 1.9647 - val_accuracy: 0.0938\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8225 - accuracy: 0.1778 \n",
      "Epoch 3: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 350s 15s/step - loss: 1.8225 - accuracy: 0.1778 - val_loss: 1.9475 - val_accuracy: 0.1250\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.8019 - accuracy: 0.1778 \n",
      "Epoch 4: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 336s 15s/step - loss: 1.8019 - accuracy: 0.1778 - val_loss: 1.9188 - val_accuracy: 0.1562\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7865 - accuracy: 0.1833 \n",
      "Epoch 5: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 336s 15s/step - loss: 1.7865 - accuracy: 0.1833 - val_loss: 1.8881 - val_accuracy: 0.1562\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7711 - accuracy: 0.1778 \n",
      "Epoch 6: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 349s 15s/step - loss: 1.7711 - accuracy: 0.1778 - val_loss: 1.8620 - val_accuracy: 0.0938\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7560 - accuracy: 0.1778 \n",
      "Epoch 7: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 366s 16s/step - loss: 1.7560 - accuracy: 0.1778 - val_loss: 1.8516 - val_accuracy: 0.1250\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7409 - accuracy: 0.1889 \n",
      "Epoch 8: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 363s 16s/step - loss: 1.7409 - accuracy: 0.1889 - val_loss: 1.8628 - val_accuracy: 0.1250\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7263 - accuracy: 0.2000 \n",
      "Epoch 9: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 365s 16s/step - loss: 1.7263 - accuracy: 0.2000 - val_loss: 1.8910 - val_accuracy: 0.1250\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7107 - accuracy: 0.2167 \n",
      "Epoch 10: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 369s 16s/step - loss: 1.7107 - accuracy: 0.2167 - val_loss: 1.9426 - val_accuracy: 0.1250\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6942 - accuracy: 0.2556 \n",
      "Epoch 11: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 366s 16s/step - loss: 1.6942 - accuracy: 0.2556 - val_loss: 2.0090 - val_accuracy: 0.1562\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6766 - accuracy: 0.2667 \n",
      "Epoch 12: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 341s 15s/step - loss: 1.6766 - accuracy: 0.2667 - val_loss: 2.0936 - val_accuracy: 0.1250\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6579 - accuracy: 0.2611 \n",
      "Epoch 13: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 340s 15s/step - loss: 1.6579 - accuracy: 0.2611 - val_loss: 2.1810 - val_accuracy: 0.0938\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6376 - accuracy: 0.2722 \n",
      "Epoch 14: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 340s 15s/step - loss: 1.6376 - accuracy: 0.2722 - val_loss: 2.2885 - val_accuracy: 0.0938\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.6166 - accuracy: 0.2889 \n",
      "Epoch 15: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 340s 15s/step - loss: 1.6166 - accuracy: 0.2889 - val_loss: 2.4224 - val_accuracy: 0.1562\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5939 - accuracy: 0.3056 \n",
      "Epoch 16: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 341s 15s/step - loss: 1.5939 - accuracy: 0.3056 - val_loss: 2.5135 - val_accuracy: 0.1250\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5696 - accuracy: 0.3333 \n",
      "Epoch 17: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 361s 16s/step - loss: 1.5696 - accuracy: 0.3333 - val_loss: 2.6354 - val_accuracy: 0.1250\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5468 - accuracy: 0.3389 \n",
      "Epoch 18: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 363s 16s/step - loss: 1.5468 - accuracy: 0.3389 - val_loss: 2.7511 - val_accuracy: 0.1250\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5220 - accuracy: 0.3333 \n",
      "Epoch 19: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 352s 15s/step - loss: 1.5220 - accuracy: 0.3333 - val_loss: 2.7061 - val_accuracy: 0.1250\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4983 - accuracy: 0.3444 \n",
      "Epoch 20: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 355s 15s/step - loss: 1.4983 - accuracy: 0.3444 - val_loss: 2.6704 - val_accuracy: 0.1250\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4749 - accuracy: 0.3667 \n",
      "Epoch 21: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 389s 17s/step - loss: 1.4749 - accuracy: 0.3667 - val_loss: 2.5893 - val_accuracy: 0.1250\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4498 - accuracy: 0.3611 \n",
      "Epoch 22: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 378s 16s/step - loss: 1.4498 - accuracy: 0.3611 - val_loss: 2.5059 - val_accuracy: 0.1250\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.4235 - accuracy: 0.3778 \n",
      "Epoch 23: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 364s 16s/step - loss: 1.4235 - accuracy: 0.3778 - val_loss: 2.4556 - val_accuracy: 0.1250\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3953 - accuracy: 0.4111 \n",
      "Epoch 24: val_accuracy did not improve from 0.21875\n",
      "23/23 [==============================] - 356s 15s/step - loss: 1.3953 - accuracy: 0.4111 - val_loss: 2.4398 - val_accuracy: 0.1250\n",
      "Epoch 25/30\n",
      "17/23 [=====================>........] - ETA: 1:33 - loss: 1.3641 - accuracy: 0.4412"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# if the training is interrupted\n",
    "model.load_weights('./sub3_training_1/cp.ckpt')\n",
    "# model.load_weights(checkpoint_path)\n",
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 30, \n",
    "                    validation_data = val_ds,\n",
    "                    callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train acc: 93.89%; val acc: 28.125%; test acc: 18.9%   # sub1 train0\n",
    "# train acc: 81.11%; val acc: 31.250%; test acc: 18.966%  # sub1 train1\n",
    "# train acc: 91.11%; val acc: 31.250%; test acc: 22.41% # sub1 train2\n",
    "# train acc: 91.67%; val acc: 31.250%;\n",
    "# need to be at least 30% to have statistical meanings\n",
    "\n",
    "# train acc: 74.44%; val acc:56.25%; test acc: 36.21% # sub2 train0\n",
    "# train acc: 67.22%; val acc:34.38%; test acc: 39.66% # sub2 train1\n",
    "# train acc: 67.78%; val acc:40.63%; test acc: 39.66% # sub2 train2-1\n",
    "\n",
    "# train acc: 65.00%; val acc: 23.33%; test acc: 20.00% # sub3 train00\n",
    "\n",
    "# train acc: 61.11%; val acc: 25.00%; test acc: 20.69% # sub4 train0\n",
    "# train acc: 52.78%; val acc: 28.125%; test acc: 22,41% # sub4 train0-1\n",
    "# train acc: 61.45%; val acc: 28.125%; test acc: 29.31% # sub4 train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  \"\"\"\n",
    "    Plotting training and validation learning curves.\n",
    "\n",
    "    Args:\n",
    "      history: model history with all the metric measures\n",
    "  \"\"\"\n",
    "  fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "  fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "  # Plot loss\n",
    "  ax1.set_title('Loss')\n",
    "  ax1.plot(history.history['loss'], label = 'train')\n",
    "  ax1.plot(history.history['val_loss'], label = 'test')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  \n",
    "  # Determine upper bound of y-axis\n",
    "  max_loss = max(history.history['loss'] + history.history['val_loss'])\n",
    "\n",
    "  ax1.set_ylim([0, np.ceil(max_loss)])\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.legend(['Train', 'Validation']) \n",
    "\n",
    "  # Plot accuracy\n",
    "  ax2.set_title('Accuracy')\n",
    "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
    "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_ylim([0, 1])\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 20s 2s/step - loss: 2.5200 - accuracy: 0.1724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.520019292831421, 'accuracy': 0.17241379618644714}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./sub3_training_1/cp.ckpt')\n",
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in test_ds:\n",
    "    y_pred_data = model.predict(x)\n",
    "    y_pred.extend(tf.argmax(y_pred_data, axis=1).numpy())\n",
    "    y_true.extend(y.numpy())\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.xticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.ylabel('True Labels')\n",
    "plt.yticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.title(f'Confusion Matrix for Subject {sub_num} on Test Dataset')\n",
    "plt.savefig(f'./illustration/Confusion Matrix for Subject {sub_num} on Test Dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in val_ds:\n",
    "    y_pred_data = model.predict(x)\n",
    "    y_pred.extend(tf.argmax(y_pred_data, axis=1).numpy())\n",
    "    y_true.extend(y.numpy())\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.xticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.ylabel('True Labels')\n",
    "plt.yticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.title(f'Confusion Matrix for Subject {sub_num} on Validation Dataset')\n",
    "plt.savefig(f'./illustration/Confusion Matrix for Subject {sub_num} on Validation Dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset): \n",
    "  \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "  \"\"\"\n",
    "  actual = [labels for _, labels in dataset.unbatch()]\n",
    "  predicted = model.predict(dataset)\n",
    "\n",
    "  actual = tf.stack(actual, axis=0)\n",
    "  predicted = tf.concat(predicted, axis=0)\n",
    "  predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "  return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(12, 12)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./CNN_3D_1.keras') # epoch around 100 times: train acc: 84.4%; val acc: 22.6%; test acc: 20.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
