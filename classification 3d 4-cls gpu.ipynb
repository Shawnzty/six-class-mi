{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda environment tf-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.io import savemat, loadmat\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import mat73\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import einops\n",
    "from scipy.io import loadmat\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "# import PIL.Image as Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# the following import is required for matplotlib < 3.2:\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg') # use a non-interactive backend such as Agg (for PNGs), PDF, SVG or PS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "# using CNN with TensorFlow core\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model, layers\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "# from tensorflow.keras import datasets, layers, models\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "# TensorFlow version: 2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_device():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './image dataset/'\n",
    "files = os.listdir(path)\n",
    "sub_num = 2\n",
    "files_head = [f for f in files if re.match(r'^sub' + str(sub_num) + '.*_1\\.jpg$', f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(fname):\n",
    "  return fname.split('_')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub(fname):\n",
    "  return fname.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_video(fname):\n",
    "    img = Image.open('./image dataset/' + fname).convert('CMYK')\n",
    "    data = np.expand_dims(np.asarray(img), axis=0)\n",
    "    for i in range(1, 7):\n",
    "        fn = fname.replace('_1.jpg',f'_{i+1}.jpg')\n",
    "        img_ = Image.open('./image dataset/' + fn).convert('CMYK')\n",
    "        arr = np.expand_dims(np.asarray(img_), axis=0)\n",
    "        data = np.concatenate((data, arr), axis=0)  # shape: (frame_num, width, height, channel)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(files):\n",
    "    ds = []\n",
    "    sub_dict = []\n",
    "    class_dict = []\n",
    "    for f in files:\n",
    "        data = integrate_video(f)\n",
    "        sub = get_sub(f)\n",
    "        cls = get_class(f)\n",
    "        ds.append(data)\n",
    "        sub_dict.append(sub)\n",
    "        class_dict.append(cls)\n",
    "\n",
    "    return ds, sub_dict, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sub_dict, class_dict = save_video(files_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_per_class(files):\n",
    "  files_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    files_for_class[class_name].append(fname)\n",
    "  return files_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_for_class = get_files_per_class(files_head)\n",
    "classes = list(files_for_class.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
    "  \"\"\" \n",
    "    Use for 3-class/in-subclass comparison\n",
    "  \"\"\"\n",
    "  files_subset = dict()\n",
    "\n",
    "  for class_name in classes:\n",
    "    class_files = files_for_class[class_name]\n",
    "    files_subset[class_name] = class_files[:files_per_class]\n",
    "\n",
    "  return files_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_class = list(classes[i] for i in [0,1,2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_subset = select_subset_of_classes(files_for_class, selected_class, FILES_PER_CLASS)\n",
    "# list(files_subset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select random train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_dict = {'front':0, 'back':1, 'left':2, 'right':3, 'up':4, 'down':5}\n",
    "# trial_dict = {'front':0, 'back':1, 'up':2, 'down':3} # D13\n",
    "trial_dict = {'front':0, 'back':1, 'left':2, 'right':3} # D12\n",
    "\n",
    "label = [trial_dict[x] for x in class_dict if x in trial_dict]\n",
    "ds = [data[i] for i in range(len(class_dict)) if class_dict[i] in trial_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(ds, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [tf.convert_to_tensor(item) for item in x_train]\n",
    "x_test = [tf.convert_to_tensor(item) for item in x_test]\n",
    "y_train = [tf.convert_to_tensor(item) for item in y_train]\n",
    "y_test = [tf.convert_to_tensor(item) for item in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = tf.data.experimental.cardinality(test_ds) # return the num of test batches\n",
    "val_ds = test_ds.take(test_batches//3)\n",
    "test_ds = test_ds.skip(test_batches//3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 369\n",
    "WIDTH = 433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "  \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "  \n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = tf.keras.layers.experimental.preprocessing.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "      \n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "      \n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, 7, HEIGHT, WIDTH, 4)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4)(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_ds))\n",
    "model.build(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 7, 369, 433, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_plus1d_9 (Conv2Plus1D)    (None, 7, 369, 433,  3936        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 7, 369, 433,  64          conv2_plus1d_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 7, 369, 433,  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "resize_video_4 (ResizeVideo)    (None, 7, 184, 216,  0           re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "residual_main_4 (ResidualMain)  (None, 7, 184, 216,  6272        resize_video_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 7, 184, 216,  0           resize_video_4[0][0]             \n",
      "                                                                 residual_main_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "resize_video_5 (ResizeVideo)    (None, 7, 92, 108, 1 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "project_3 (Project)             (None, 7, 92, 108, 3 608         resize_video_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "residual_main_5 (ResidualMain)  (None, 7, 92, 108, 3 20224       resize_video_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 7, 92, 108, 3 0           project_3[0][0]                  \n",
      "                                                                 residual_main_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "resize_video_6 (ResizeVideo)    (None, 7, 46, 54, 32 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "project_4 (Project)             (None, 7, 46, 54, 64 2240        resize_video_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "residual_main_6 (ResidualMain)  (None, 7, 46, 54, 64 80384       resize_video_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 7, 46, 54, 64 0           project_4[0][0]                  \n",
      "                                                                 residual_main_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "resize_video_7 (ResizeVideo)    (None, 7, 23, 27, 64 0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "project_5 (Project)             (None, 7, 23, 27, 12 8576        resize_video_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "residual_main_7 (ResidualMain)  (None, 7, 23, 27, 12 320512      resize_video_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 7, 23, 27, 12 0           project_5[0][0]                  \n",
      "                                                                 residual_main_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling3d_1 (Glo (None, 128)          0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           global_average_pooling3d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            516         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 443,332\n",
      "Trainable params: 443,300\n",
      "Non-trainable params: 32\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./sub3_4-cls-12_0/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True, # save the entire model or weights only\n",
    "                                                 verbose=1,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max',  # max for acc, min for loss\n",
    "                                                 save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "15/15 [==============================] - 15s 744ms/step - loss: 1.5735 - accuracy: 0.2583 - val_loss: 1.8385 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.18750, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 2/60\n",
      "15/15 [==============================] - 10s 655ms/step - loss: 1.5721 - accuracy: 0.2583 - val_loss: 1.6818 - val_accuracy: 0.0625\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.18750\n",
      "Epoch 3/60\n",
      "15/15 [==============================] - 10s 687ms/step - loss: 1.6032 - accuracy: 0.1833 - val_loss: 1.5694 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.18750 to 0.25000, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 4/60\n",
      "15/15 [==============================] - 10s 676ms/step - loss: 1.4550 - accuracy: 0.2417 - val_loss: 1.4843 - val_accuracy: 0.1250\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.25000\n",
      "Epoch 5/60\n",
      "15/15 [==============================] - 10s 682ms/step - loss: 1.4322 - accuracy: 0.2167 - val_loss: 1.4754 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.25000\n",
      "Epoch 6/60\n",
      "15/15 [==============================] - 10s 666ms/step - loss: 1.4117 - accuracy: 0.2333 - val_loss: 1.5032 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.25000\n",
      "Epoch 7/60\n",
      "15/15 [==============================] - 10s 693ms/step - loss: 1.3906 - accuracy: 0.2333 - val_loss: 1.4546 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.25000\n",
      "Epoch 8/60\n",
      "15/15 [==============================] - 10s 698ms/step - loss: 1.3789 - accuracy: 0.2333 - val_loss: 1.4953 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.25000\n",
      "Epoch 9/60\n",
      "15/15 [==============================] - 10s 686ms/step - loss: 1.3648 - accuracy: 0.2583 - val_loss: 1.4961 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.25000\n",
      "Epoch 10/60\n",
      "15/15 [==============================] - 10s 691ms/step - loss: 1.3546 - accuracy: 0.2583 - val_loss: 1.5295 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.25000\n",
      "Epoch 11/60\n",
      "15/15 [==============================] - 10s 673ms/step - loss: 1.3449 - accuracy: 0.2750 - val_loss: 1.5242 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.25000\n",
      "Epoch 12/60\n",
      "15/15 [==============================] - 10s 693ms/step - loss: 1.3340 - accuracy: 0.2833 - val_loss: 1.5636 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.25000\n",
      "Epoch 13/60\n",
      "15/15 [==============================] - 11s 730ms/step - loss: 1.3249 - accuracy: 0.3083 - val_loss: 1.5484 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.25000\n",
      "Epoch 14/60\n",
      "15/15 [==============================] - 11s 767ms/step - loss: 1.3153 - accuracy: 0.3083 - val_loss: 1.5728 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.25000\n",
      "Epoch 15/60\n",
      "15/15 [==============================] - 11s 758ms/step - loss: 1.3050 - accuracy: 0.3083 - val_loss: 1.5819 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.25000\n",
      "Epoch 16/60\n",
      "15/15 [==============================] - 10s 676ms/step - loss: 1.2960 - accuracy: 0.3417 - val_loss: 1.5764 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.25000\n",
      "Epoch 17/60\n",
      "15/15 [==============================] - 10s 674ms/step - loss: 1.2860 - accuracy: 0.3417 - val_loss: 1.5629 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.25000\n",
      "Epoch 18/60\n",
      "15/15 [==============================] - 10s 656ms/step - loss: 1.2754 - accuracy: 0.3333 - val_loss: 1.5460 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.25000\n",
      "Epoch 19/60\n",
      "15/15 [==============================] - 10s 658ms/step - loss: 1.2645 - accuracy: 0.3500 - val_loss: 1.5545 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.25000\n",
      "Epoch 20/60\n",
      "15/15 [==============================] - 10s 656ms/step - loss: 1.2532 - accuracy: 0.3667 - val_loss: 1.5462 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.25000\n",
      "Epoch 21/60\n",
      "15/15 [==============================] - 10s 653ms/step - loss: 1.2417 - accuracy: 0.3750 - val_loss: 1.5058 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.25000\n",
      "Epoch 22/60\n",
      "15/15 [==============================] - 10s 669ms/step - loss: 1.2287 - accuracy: 0.3667 - val_loss: 1.5307 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.25000\n",
      "Epoch 23/60\n",
      "15/15 [==============================] - 11s 726ms/step - loss: 1.2155 - accuracy: 0.3750 - val_loss: 1.4753 - val_accuracy: 0.3125\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.25000 to 0.31250, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 24/60\n",
      "15/15 [==============================] - 11s 706ms/step - loss: 1.2020 - accuracy: 0.3833 - val_loss: 1.4673 - val_accuracy: 0.3125\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.31250\n",
      "Epoch 25/60\n",
      "15/15 [==============================] - 8s 568ms/step - loss: 1.1872 - accuracy: 0.4000 - val_loss: 1.5384 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.31250\n",
      "Epoch 26/60\n",
      "15/15 [==============================] - 8s 569ms/step - loss: 1.1748 - accuracy: 0.4250 - val_loss: 1.3895 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.31250 to 0.43750, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 27/60\n",
      "15/15 [==============================] - 9s 581ms/step - loss: 1.1607 - accuracy: 0.4583 - val_loss: 2.1555 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.43750\n",
      "Epoch 28/60\n",
      "15/15 [==============================] - 8s 549ms/step - loss: 1.1594 - accuracy: 0.4167 - val_loss: 1.3356 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.43750\n",
      "Epoch 29/60\n",
      "15/15 [==============================] - 8s 533ms/step - loss: 1.1355 - accuracy: 0.4750 - val_loss: 2.6125 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.43750\n",
      "Epoch 30/60\n",
      "15/15 [==============================] - 8s 531ms/step - loss: 1.1219 - accuracy: 0.4583 - val_loss: 1.2931 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.43750\n",
      "Epoch 31/60\n",
      "15/15 [==============================] - 8s 535ms/step - loss: 1.0957 - accuracy: 0.4750 - val_loss: 1.8993 - val_accuracy: 0.3125\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.43750\n",
      "Epoch 32/60\n",
      "15/15 [==============================] - 9s 573ms/step - loss: 1.0798 - accuracy: 0.5000 - val_loss: 1.3041 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.43750\n",
      "Epoch 33/60\n",
      "15/15 [==============================] - 8s 564ms/step - loss: 1.0606 - accuracy: 0.5000 - val_loss: 1.3773 - val_accuracy: 0.3750\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.43750\n",
      "Epoch 34/60\n",
      "15/15 [==============================] - 9s 578ms/step - loss: 1.0433 - accuracy: 0.5083 - val_loss: 1.2684 - val_accuracy: 0.3750\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.43750\n",
      "Epoch 35/60\n",
      "15/15 [==============================] - 9s 577ms/step - loss: 1.0241 - accuracy: 0.5250 - val_loss: 1.2362 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.43750\n",
      "Epoch 36/60\n",
      "15/15 [==============================] - 8s 531ms/step - loss: 1.0057 - accuracy: 0.5333 - val_loss: 1.1899 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.43750\n",
      "Epoch 37/60\n",
      "15/15 [==============================] - 9s 588ms/step - loss: 0.9832 - accuracy: 0.5583 - val_loss: 1.1607 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.43750\n",
      "Epoch 38/60\n",
      "15/15 [==============================] - 11s 758ms/step - loss: 0.9317 - accuracy: 0.6167 - val_loss: 1.1766 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.43750\n",
      "Epoch 39/60\n",
      "15/15 [==============================] - 13s 874ms/step - loss: 0.9001 - accuracy: 0.5917 - val_loss: 1.2370 - val_accuracy: 0.3125\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.43750\n",
      "Epoch 40/60\n",
      "15/15 [==============================] - 13s 878ms/step - loss: 0.8831 - accuracy: 0.6000 - val_loss: 1.2415 - val_accuracy: 0.3125\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.43750\n",
      "Epoch 41/60\n",
      "15/15 [==============================] - 13s 876ms/step - loss: 0.8835 - accuracy: 0.6000 - val_loss: 1.1723 - val_accuracy: 0.3750\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.43750\n",
      "Epoch 42/60\n",
      "15/15 [==============================] - 13s 881ms/step - loss: 0.8391 - accuracy: 0.6583 - val_loss: 1.2176 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.43750\n",
      "Epoch 43/60\n",
      "15/15 [==============================] - 13s 872ms/step - loss: 0.7837 - accuracy: 0.6667 - val_loss: 1.3633 - val_accuracy: 0.3750\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.43750\n",
      "Epoch 44/60\n",
      "15/15 [==============================] - 13s 876ms/step - loss: 0.7522 - accuracy: 0.7250 - val_loss: 1.3218 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.43750\n",
      "Epoch 45/60\n",
      "15/15 [==============================] - 13s 871ms/step - loss: 0.7471 - accuracy: 0.6833 - val_loss: 1.3832 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.43750\n",
      "Epoch 46/60\n",
      "15/15 [==============================] - 13s 880ms/step - loss: 0.7151 - accuracy: 0.7000 - val_loss: 1.4468 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.43750\n",
      "Epoch 47/60\n",
      "15/15 [==============================] - 13s 875ms/step - loss: 0.7252 - accuracy: 0.7000 - val_loss: 1.4981 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.43750 to 0.50000, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 48/60\n",
      "15/15 [==============================] - 13s 874ms/step - loss: 0.7482 - accuracy: 0.6750 - val_loss: 1.8102 - val_accuracy: 0.3750\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.50000\n",
      "Epoch 49/60\n",
      "15/15 [==============================] - 13s 878ms/step - loss: 0.8812 - accuracy: 0.6167 - val_loss: 1.4618 - val_accuracy: 0.3750\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.50000\n",
      "Epoch 50/60\n",
      "15/15 [==============================] - 13s 882ms/step - loss: 0.7472 - accuracy: 0.6500 - val_loss: 1.2619 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.50000\n",
      "Epoch 51/60\n",
      "15/15 [==============================] - 13s 881ms/step - loss: 0.9498 - accuracy: 0.6000 - val_loss: 2.9799 - val_accuracy: 0.1875\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.50000\n",
      "Epoch 52/60\n",
      "15/15 [==============================] - 13s 873ms/step - loss: 0.9875 - accuracy: 0.6000 - val_loss: 1.4210 - val_accuracy: 0.4375\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.50000\n",
      "Epoch 53/60\n",
      "15/15 [==============================] - 13s 879ms/step - loss: 0.6956 - accuracy: 0.7667 - val_loss: 1.1212 - val_accuracy: 0.5625\n",
      "\n",
      "Epoch 00053: val_accuracy improved from 0.50000 to 0.56250, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 54/60\n",
      "15/15 [==============================] - 13s 879ms/step - loss: 0.6542 - accuracy: 0.7583 - val_loss: 1.1513 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00054: val_accuracy improved from 0.56250 to 0.62500, saving model to ./sub3_4-cls-12_0\\cp.ckpt\n",
      "Epoch 55/60\n",
      "15/15 [==============================] - 13s 873ms/step - loss: 0.6102 - accuracy: 0.7750 - val_loss: 1.1907 - val_accuracy: 0.5625\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.62500\n",
      "Epoch 56/60\n",
      "15/15 [==============================] - 13s 877ms/step - loss: 0.5999 - accuracy: 0.8000 - val_loss: 1.0608 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.62500\n",
      "Epoch 57/60\n",
      "15/15 [==============================] - 13s 885ms/step - loss: 0.6124 - accuracy: 0.7750 - val_loss: 1.0685 - val_accuracy: 0.6250\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.62500\n",
      "Epoch 58/60\n",
      "15/15 [==============================] - 13s 874ms/step - loss: 0.5849 - accuracy: 0.7500 - val_loss: 1.2694 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.62500\n",
      "Epoch 59/60\n",
      "15/15 [==============================] - 13s 876ms/step - loss: 0.5367 - accuracy: 0.8000 - val_loss: 1.2576 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.62500\n",
      "Epoch 60/60\n",
      "15/15 [==============================] - 13s 877ms/step - loss: 0.5381 - accuracy: 0.8167 - val_loss: 1.1838 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.62500\n"
     ]
    }
   ],
   "source": [
    "# print('hi')\n",
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 60,\n",
    "                    validation_data = val_ds,\n",
    "                    callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the training is interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if the training is interrupted\n",
    "# model.load_weights('./sub3_4-cls-12_0/cp.ckpt')\n",
    "# model.load_weights(checkpoint_path)\n",
    "# history = model.fit(x = train_ds,\n",
    "#                     epochs = 40, \n",
    "#                     validation_data = val_ds,\n",
    "#                     callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train acc: 100%; val acc: 43.75%; test acc: 36.36%   # sub1 4-cls-13 train0\n",
    "\n",
    "# train acc: 95.00%; val acc: 62.50%; test acc: 43.18% # sub2 4-cls-13 train1\n",
    "# train acc: \n",
    "\n",
    "# train acc: 80.00%; val acc: 50.00%; test acc: 20.45% # sub3 4-cls-13 train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 315ms/step - loss: 1.5118 - accuracy: 0.4318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.5117790699005127, 'accuracy': 0.4318181872367859}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./sub3_4-cls-12_0/cp.ckpt')\n",
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in test_ds:\n",
    "    y_pred_data = model.predict(x)\n",
    "    y_pred.extend(tf.argmax(y_pred_data, axis=1).numpy())\n",
    "    y_true.extend(y.numpy())\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.xticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.ylabel('True Labels')\n",
    "plt.yticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.title(f'Confusion Matrix for Subject {sub_num} on Test Dataset')\n",
    "plt.savefig(f'./illustration/Confusion Matrix for Subject {sub_num} on Test Dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for x, y in val_ds:\n",
    "    y_pred_data = model.predict(x)\n",
    "    y_pred.extend(tf.argmax(y_pred_data, axis=1).numpy())\n",
    "    y_true.extend(y.numpy())\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.xticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.ylabel('True Labels')\n",
    "plt.yticks([0, 1, 2, 3, 4, 5], ['front', 'back', 'left', 'right', 'up', 'down'])\n",
    "plt.title(f'Confusion Matrix for Subject {sub_num} on Validation Dataset')\n",
    "plt.savefig(f'./illustration/Confusion Matrix for Subject {sub_num} on Validation Dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset): \n",
    "  \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "  \"\"\"\n",
    "  actual = [labels for _, labels in dataset.unbatch()]\n",
    "  predicted = model.predict(dataset)\n",
    "\n",
    "  actual = tf.stack(actual, axis=0)\n",
    "  predicted = tf.concat(predicted, axis=0)\n",
    "  predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "  return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(12, 12)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./CNN_3D_1.keras') # epoch around 100 times: train acc: 84.4%; val acc: 22.6%; test acc: 20.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
